From dea96d76e50e9aaaf2f2cf00925a18551d5e881e Mon Sep 17 00:00:00 2001
From: Mike Banon <mikebdp2@gmail.com>
Date: Tue, 10 Dec 2019 16:58:42 +0000
Subject: [PATCH] Modify ./include/asm-mips/ files according to WR841N(D)v9

---
 include/asm-mips/addrspace.h |  6 +++-
 include/asm-mips/bitops.h    | 68 +++++++++++++++++++-----------------
 include/asm-mips/io.h        | 23 ++++++------
 include/asm-mips/mipsregs.h  |  9 +++--
 include/asm-mips/processor.h |  5 ++-
 include/asm-mips/system.h    |  9 +++--
 6 files changed, 70 insertions(+), 50 deletions(-)

diff --git a/include/asm-mips/addrspace.h b/include/asm-mips/addrspace.h
index 0e6abd7d0f..883d406359 100644
--- a/include/asm-mips/addrspace.h
+++ b/include/asm-mips/addrspace.h
@@ -48,7 +48,11 @@
 /* We use a 36 bit physical address map here and
    cannot access physical memory directly from core */
 #define UNCACHED_SDRAM(a) (((unsigned long)(a)) | 0x20000000)
-#else	/* !CONFIG_AU1X00 */
+/* cu570m start */
+#elif defined(CONFIG_AR7100) || defined(CONFIG_AR7240) || defined(CONFIG_ATHEROS)
+#define UNCACHED_SDRAM(a)   KSEG1ADDR((a))
+#else	/* !CONFIG_AR7100 */ /* AU1X00 -> AR7100 - cu570m */
+/* cu570m end */
 #define UNCACHED_SDRAM(a) KSEG1ADDR(a)
 #endif	/* CONFIG_AU1X00 */
 #endif	/* __ASSEMBLY__ */
diff --git a/include/asm-mips/bitops.h b/include/asm-mips/bitops.h
index 56d7225bb7..1dec59e50d 100644
--- a/include/asm-mips/bitops.h
+++ b/include/asm-mips/bitops.h
@@ -6,6 +6,9 @@
  * Copyright (c) 1994 - 1997, 1999, 2000  Ralf Baechle (ralf@gnu.org)
  * Copyright (c) 2000  Silicon Graphics, Inc.
  */
+
+/* extern -> static - cu570m */
+
 #ifndef _ASM_BITOPS_H
 #define _ASM_BITOPS_H
 
@@ -60,7 +63,7 @@
  * Note that @nr may be almost arbitrarily large; this function is not
  * restricted to acting on a single-word quantity.
  */
-extern __inline__ void
+static __inline__ void
 set_bit(int nr, volatile void *addr)
 {
 	unsigned long *m = ((unsigned long *) addr) + (nr >> 5);
@@ -84,7 +87,7 @@ set_bit(int nr, volatile void *addr)
  * If it's called on the same region of memory simultaneously, the effect
  * may be that only one operation succeeds.
  */
-extern __inline__ void __set_bit(int nr, volatile void * addr)
+static __inline__ void __set_bit(int nr, volatile void * addr)
 {
 	unsigned long * m = ((unsigned long *) addr) + (nr >> 5);
 
@@ -101,7 +104,7 @@ extern __inline__ void __set_bit(int nr, volatile void * addr)
  * you should call smp_mb__before_clear_bit() and/or smp_mb__after_clear_bit()
  * in order to ensure changes are visible on other processors.
  */
-extern __inline__ void
+static __inline__ void
 clear_bit(int nr, volatile void *addr)
 {
 	unsigned long *m = ((unsigned long *) addr) + (nr >> 5);
@@ -125,7 +128,7 @@ clear_bit(int nr, volatile void *addr)
  * Note that @nr may be almost arbitrarily large; this function is not
  * restricted to acting on a single-word quantity.
  */
-extern __inline__ void
+static __inline__ void
 change_bit(int nr, volatile void *addr)
 {
 	unsigned long *m = ((unsigned long *) addr) + (nr >> 5);
@@ -149,7 +152,7 @@ change_bit(int nr, volatile void *addr)
  * If it's called on the same region of memory simultaneously, the effect
  * may be that only one operation succeeds.
  */
-extern __inline__ void __change_bit(int nr, volatile void * addr)
+static __inline__ void __change_bit(int nr, volatile void * addr)
 {
 	unsigned long * m = ((unsigned long *) addr) + (nr >> 5);
 
@@ -164,7 +167,7 @@ extern __inline__ void __change_bit(int nr, volatile void * addr)
  * This operation is atomic and cannot be reordered.
  * It also implies a memory barrier.
  */
-extern __inline__ int
+static __inline__ int
 test_and_set_bit(int nr, volatile void *addr)
 {
 	unsigned long *m = ((unsigned long *) addr) + (nr >> 5);
@@ -194,7 +197,7 @@ test_and_set_bit(int nr, volatile void *addr)
  * If two examples of this operation race, one can appear to succeed
  * but actually fail.  You must protect multiple accesses with a lock.
  */
-extern __inline__ int __test_and_set_bit(int nr, volatile void * addr)
+static __inline__ int __test_and_set_bit(int nr, volatile void * addr)
 {
 	int mask, retval;
 	volatile int *a = addr;
@@ -215,7 +218,7 @@ extern __inline__ int __test_and_set_bit(int nr, volatile void * addr)
  * This operation is atomic and cannot be reordered.
  * It also implies a memory barrier.
  */
-extern __inline__ int
+static __inline__ int
 test_and_clear_bit(int nr, volatile void *addr)
 {
 	unsigned long *m = ((unsigned long *) addr) + (nr >> 5);
@@ -246,7 +249,7 @@ test_and_clear_bit(int nr, volatile void *addr)
  * If two examples of this operation race, one can appear to succeed
  * but actually fail.  You must protect multiple accesses with a lock.
  */
-extern __inline__ int __test_and_clear_bit(int nr, volatile void * addr)
+static __inline__ int __test_and_clear_bit(int nr, volatile void * addr)
 {
 	int	mask, retval;
 	volatile int	*a = addr;
@@ -267,7 +270,7 @@ extern __inline__ int __test_and_clear_bit(int nr, volatile void * addr)
  * This operation is atomic and cannot be reordered.
  * It also implies a memory barrier.
  */
-extern __inline__ int
+static __inline__ int
 test_and_change_bit(int nr, volatile void *addr)
 {
 	unsigned long *m = ((unsigned long *) addr) + (nr >> 5);
@@ -297,7 +300,7 @@ test_and_change_bit(int nr, volatile void *addr)
  * If two examples of this operation race, one can appear to succeed
  * but actually fail.  You must protect multiple accesses with a lock.
  */
-extern __inline__ int __test_and_change_bit(int nr, volatile void * addr)
+static __inline__ int __test_and_change_bit(int nr, volatile void * addr)
 {
 	int	mask, retval;
 	volatile int	*a = addr;
@@ -322,7 +325,7 @@ extern __inline__ int __test_and_change_bit(int nr, volatile void * addr)
  * Note that @nr may be almost arbitrarily large; this function is not
  * restricted to acting on a single-word quantity.
  */
-extern __inline__ void set_bit(int nr, volatile void * addr)
+static __inline__ void set_bit(int nr, volatile void * addr)
 {
 	int	mask;
 	volatile int	*a = addr;
@@ -344,7 +347,7 @@ extern __inline__ void set_bit(int nr, volatile void * addr)
  * If it's called on the same region of memory simultaneously, the effect
  * may be that only one operation succeeds.
  */
-extern __inline__ void __set_bit(int nr, volatile void * addr)
+static __inline__ void __set_bit(int nr, volatile void * addr)
 {
 	int	mask;
 	volatile int	*a = addr;
@@ -364,7 +367,7 @@ extern __inline__ void __set_bit(int nr, volatile void * addr)
  * you should call smp_mb__before_clear_bit() and/or smp_mb__after_clear_bit()
  * in order to ensure changes are visible on other processors.
  */
-extern __inline__ void clear_bit(int nr, volatile void * addr)
+static __inline__ void clear_bit(int nr, volatile void * addr)
 {
 	int	mask;
 	volatile int	*a = addr;
@@ -386,7 +389,7 @@ extern __inline__ void clear_bit(int nr, volatile void * addr)
  * Note that @nr may be almost arbitrarily large; this function is not
  * restricted to acting on a single-word quantity.
  */
-extern __inline__ void change_bit(int nr, volatile void * addr)
+static __inline__ void change_bit(int nr, volatile void * addr)
 {
 	int	mask;
 	volatile int	*a = addr;
@@ -408,7 +411,7 @@ extern __inline__ void change_bit(int nr, volatile void * addr)
  * If it's called on the same region of memory simultaneously, the effect
  * may be that only one operation succeeds.
  */
-extern __inline__ void __change_bit(int nr, volatile void * addr)
+static __inline__ void __change_bit(int nr, volatile void * addr)
 {
 	unsigned long * m = ((unsigned long *) addr) + (nr >> 5);
 
@@ -423,7 +426,7 @@ extern __inline__ void __change_bit(int nr, volatile void * addr)
  * This operation is atomic and cannot be reordered.
  * It also implies a memory barrier.
  */
-extern __inline__ int test_and_set_bit(int nr, volatile void * addr)
+static __inline__ int test_and_set_bit(int nr, volatile void * addr)
 {
 	int	mask, retval;
 	volatile int	*a = addr;
@@ -448,7 +451,7 @@ extern __inline__ int test_and_set_bit(int nr, volatile void * addr)
  * If two examples of this operation race, one can appear to succeed
  * but actually fail.  You must protect multiple accesses with a lock.
  */
-extern __inline__ int __test_and_set_bit(int nr, volatile void * addr)
+static __inline__ int __test_and_set_bit(int nr, volatile void * addr)
 {
 	int	mask, retval;
 	volatile int	*a = addr;
@@ -469,7 +472,7 @@ extern __inline__ int __test_and_set_bit(int nr, volatile void * addr)
  * This operation is atomic and cannot be reordered.
  * It also implies a memory barrier.
  */
-extern __inline__ int test_and_clear_bit(int nr, volatile void * addr)
+static __inline__ int test_and_clear_bit(int nr, volatile void * addr)
 {
 	int	mask, retval;
 	volatile int	*a = addr;
@@ -494,7 +497,7 @@ extern __inline__ int test_and_clear_bit(int nr, volatile void * addr)
  * If two examples of this operation race, one can appear to succeed
  * but actually fail.  You must protect multiple accesses with a lock.
  */
-extern __inline__ int __test_and_clear_bit(int nr, volatile void * addr)
+static __inline__ int __test_and_clear_bit(int nr, volatile void * addr)
 {
 	int	mask, retval;
 	volatile int	*a = addr;
@@ -515,7 +518,7 @@ extern __inline__ int __test_and_clear_bit(int nr, volatile void * addr)
  * This operation is atomic and cannot be reordered.
  * It also implies a memory barrier.
  */
-extern __inline__ int test_and_change_bit(int nr, volatile void * addr)
+static __inline__ int test_and_change_bit(int nr, volatile void * addr)
 {
 	int	mask, retval;
 	volatile int	*a = addr;
@@ -540,7 +543,7 @@ extern __inline__ int test_and_change_bit(int nr, volatile void * addr)
  * If two examples of this operation race, one can appear to succeed
  * but actually fail.  You must protect multiple accesses with a lock.
  */
-extern __inline__ int __test_and_change_bit(int nr, volatile void * addr)
+static __inline__ int __test_and_change_bit(int nr, volatile void * addr)
 {
 	int	mask, retval;
 	volatile int	*a = addr;
@@ -565,7 +568,7 @@ extern __inline__ int __test_and_change_bit(int nr, volatile void * addr)
  * @nr: bit number to test
  * @addr: Address to start counting from
  */
-extern __inline__ int test_bit(int nr, volatile void *addr)
+static __inline__ int test_bit(int nr, volatile void *addr)
 {
 	return ((1UL << (nr & 31)) & (((const unsigned int *) addr)[nr >> 5])) != 0;
 }
@@ -582,7 +585,7 @@ extern __inline__ int test_bit(int nr, volatile void *addr)
  * Returns the bit-number of the first zero bit, not the number of the byte
  * containing a bit.
  */
-extern __inline__ int find_first_zero_bit (void *addr, unsigned size)
+static __inline__ int find_first_zero_bit (void *addr, unsigned size)
 {
 	unsigned long dummy;
 	int res;
@@ -633,7 +636,7 @@ extern __inline__ int find_first_zero_bit (void *addr, unsigned size)
  * @offset: The bitnumber to start searching at
  * @size: The maximum size to search
  */
-extern __inline__ int find_next_zero_bit (void * addr, int size, int offset)
+static __inline__ int find_next_zero_bit (void * addr, int size, int offset)
 {
 	unsigned int *p = ((unsigned int *) addr) + (offset >> 5);
 	int set = 0, bit = offset & 31, res;
@@ -679,7 +682,7 @@ extern __inline__ int find_next_zero_bit (void * addr, int size, int offset)
  *
  * Undefined if no zero exists, so code should check against ~0UL first.
  */
-extern __inline__ unsigned long ffz(unsigned long word)
+static __inline__ unsigned long ffz(unsigned long word)
 {
 	unsigned int	__res;
 	unsigned int	mask = 1;
@@ -736,7 +739,7 @@ extern __inline__ unsigned long ffz(unsigned long word)
  * @offset: The bitnumber to start searching at
  * @size: The maximum size to search
  */
-extern __inline__ int find_next_zero_bit(void *addr, int size, int offset)
+static __inline__ int find_next_zero_bit(void *addr, int size, int offset)
 {
 	unsigned long *p = ((unsigned long *) addr) + (offset >> 5);
 	unsigned long result = offset & ~31UL;
@@ -794,9 +797,9 @@ extern int find_first_zero_bit (void *addr, unsigned size);
 #endif /* (__MIPSEB__) */
 
 /* Now for the ext2 filesystem bit operations and helper routines. */
-
+#if defined(CONFIG_CMD_EXT2) /* cu570m */
 #ifdef __MIPSEB__
-extern __inline__ int ext2_set_bit(int nr, void * addr)
+static __inline__ int ext2_set_bit(int nr, void * addr)
 {
 	int		mask, retval, flags;
 	unsigned char	*ADDR = (unsigned char *) addr;
@@ -810,7 +813,7 @@ extern __inline__ int ext2_set_bit(int nr, void * addr)
 	return retval;
 }
 
-extern __inline__ int ext2_clear_bit(int nr, void * addr)
+static __inline__ int ext2_clear_bit(int nr, void * addr)
 {
 	int		mask, retval, flags;
 	unsigned char	*ADDR = (unsigned char *) addr;
@@ -824,7 +827,7 @@ extern __inline__ int ext2_clear_bit(int nr, void * addr)
 	return retval;
 }
 
-extern __inline__ int ext2_test_bit(int nr, const void * addr)
+static __inline__ int ext2_test_bit(int nr, const void * addr)
 {
 	int			mask;
 	const unsigned char	*ADDR = (const unsigned char *) addr;
@@ -837,7 +840,7 @@ extern __inline__ int ext2_test_bit(int nr, const void * addr)
 #define ext2_find_first_zero_bit(addr, size) \
 	ext2_find_next_zero_bit((addr), (size), 0)
 
-extern __inline__ unsigned long ext2_find_next_zero_bit(void *addr, unsigned long size, unsigned long offset)
+static __inline__ unsigned long ext2_find_next_zero_bit(void *addr, unsigned long size, unsigned long offset)
 {
 	unsigned long *p = ((unsigned long *) addr) + (offset >> 5);
 	unsigned long result = offset & ~31UL;
@@ -908,5 +911,6 @@ found_middle:
 #define minix_test_and_clear_bit(nr,addr) test_and_clear_bit(nr,addr)
 #define minix_test_bit(nr,addr) test_bit(nr,addr)
 #define minix_find_first_zero_bit(addr,size) find_first_zero_bit(addr,size)
+#endif /* (CONFIG_CMD_EXT2) */ /* cu570m */
 
 #endif /* _ASM_BITOPS_H */
diff --git a/include/asm-mips/io.h b/include/asm-mips/io.h
index e27d1f159d..fc807968eb 100644
--- a/include/asm-mips/io.h
+++ b/include/asm-mips/io.h
@@ -8,6 +8,9 @@
  * Copyright (C) 1999, 2000 Silicon Graphics, Inc.
  * Copyright (C) 2000 FSMLabs, Inc.
  */
+
+/* extern -> static - cu570m */
+
 #ifndef _ASM_IO_H
 #define _ASM_IO_H
 
@@ -118,12 +121,12 @@ static inline void set_io_port_base(unsigned long base)
  * Change virtual addresses to physical addresses and vv.
  * These are trivial on the 1:1 Linux/MIPS mapping
  */
-extern inline unsigned long virt_to_phys(volatile void * address)
+static inline unsigned long virt_to_phys(volatile void * address)
 {
 	return PHYSADDR(address);
 }
 
-extern inline void * phys_to_virt(unsigned long address)
+static inline void * phys_to_virt(unsigned long address)
 {
 	return (void *)KSEG0ADDR(address);
 }
@@ -131,12 +134,12 @@ extern inline void * phys_to_virt(unsigned long address)
 /*
  * IO bus memory addresses are also 1:1 with the physical address
  */
-extern inline unsigned long virt_to_bus(volatile void * address)
+static inline unsigned long virt_to_bus(volatile void * address)
 {
 	return PHYSADDR(address);
 }
 
-extern inline void * bus_to_virt(unsigned long address)
+static inline void * bus_to_virt(unsigned long address)
 {
 	return (void *)KSEG0ADDR(address);
 }
@@ -150,12 +153,12 @@ extern unsigned long isa_slot_offset;
 extern void * __ioremap(unsigned long offset, unsigned long size, unsigned long flags);
 
 #if 0
-extern inline void *ioremap(unsigned long offset, unsigned long size)
+static inline void *ioremap(unsigned long offset, unsigned long size)
 {
 	return __ioremap(offset, size, _CACHE_UNCACHED);
 }
 
-extern inline void *ioremap_nocache(unsigned long offset, unsigned long size)
+static inline void *ioremap_nocache(unsigned long offset, unsigned long size)
 {
 	return __ioremap(offset, size, _CACHE_UNCACHED);
 }
@@ -238,7 +241,7 @@ out:
  */
 
 #define __OUT1(s) \
-extern inline void __out##s(unsigned int value, unsigned int port) {
+static inline void __out##s(unsigned int value, unsigned int port) {
 
 #define __OUT2(m) \
 __asm__ __volatile__ ("s" #m "\t%0,%1(%2)"
@@ -252,7 +255,7 @@ __OUT1(s##c_p) __OUT2(m) : : "r" (__ioswab##w(value)), "ir" (port), "r" (mips_io
 	SLOW_DOWN_IO; }
 
 #define __IN1(t,s) \
-extern __inline__ t __in##s(unsigned int port) { t _v;
+static __inline__ t __in##s(unsigned int port) { t _v;
 
 /*
  * Required nops will be inserted by the assembler
@@ -267,7 +270,7 @@ __IN1(t,s##_p) __IN2(m) : "=r" (_v) : "i" (0), "r" (mips_io_port_base+port)); SL
 __IN1(t,s##c_p) __IN2(m) : "=r" (_v) : "ir" (port), "r" (mips_io_port_base)); SLOW_DOWN_IO; return __ioswab##w(_v); }
 
 #define __INS1(s) \
-extern inline void __ins##s(unsigned int port, void * addr, unsigned long count) {
+static inline void __ins##s(unsigned int port, void * addr, unsigned long count) {
 
 #define __INS2(m) \
 if (count) \
@@ -295,7 +298,7 @@ __INS1(s##c) __INS2(m) \
 	: "$1");}
 
 #define __OUTS1(s) \
-extern inline void __outs##s(unsigned int port, const void * addr, unsigned long count) {
+static inline void __outs##s(unsigned int port, const void * addr, unsigned long count) {
 
 #define __OUTS2(m) \
 if (count) \
diff --git a/include/asm-mips/mipsregs.h b/include/asm-mips/mipsregs.h
index 0586c53d3b..495dadcffc 100644
--- a/include/asm-mips/mipsregs.h
+++ b/include/asm-mips/mipsregs.h
@@ -10,6 +10,9 @@
  * Copyright (C) 2000 MIPS Technologies, Inc.  All rights reserved.
  * Copyright (C) 2003  Maciej W. Rozycki
  */
+
+/* extern -> static - cu570m */
+
 #ifndef _ASM_MIPSREGS_H
 #define _ASM_MIPSREGS_H
 
@@ -276,7 +279,7 @@
  * Mostly used to access the interrupt bits.
  */
 #define __BUILD_SET_CP0(name,register)                          \
-extern __inline__ unsigned int                                  \
+static __inline__ unsigned int                                  \
 set_cp0_##name(unsigned int set)				\
 {                                                               \
 	unsigned int res;                                       \
@@ -288,7 +291,7 @@ set_cp0_##name(unsigned int set)				\
 	return res;                                             \
 }								\
 								\
-extern __inline__ unsigned int                                  \
+static __inline__ unsigned int                                  \
 clear_cp0_##name(unsigned int clear)				\
 {                                                               \
 	unsigned int res;                                       \
@@ -300,7 +303,7 @@ clear_cp0_##name(unsigned int clear)				\
 	return res;                                             \
 }								\
 								\
-extern __inline__ unsigned int                                  \
+static __inline__ unsigned int                                  \
 change_cp0_##name(unsigned int change, unsigned int new)	\
 {                                                               \
 	unsigned int res;                                       \
diff --git a/include/asm-mips/processor.h b/include/asm-mips/processor.h
index 6838aee98c..6ad71f749e 100644
--- a/include/asm-mips/processor.h
+++ b/include/asm-mips/processor.h
@@ -8,6 +8,9 @@
  * Copyright (C) 1996 Paul M. Antoine
  * Copyright (C) 1999 Silicon Graphics, Inc.
  */
+
+/* extern -> static - cu570m */
+
 #ifndef _ASM_PROCESSOR_H
 #define _ASM_PROCESSOR_H
 
@@ -219,7 +222,7 @@ extern int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags);
 /*
  * Return saved PC of a blocked thread.
  */
-extern inline unsigned long thread_saved_pc(struct thread_struct *t)
+static inline unsigned long thread_saved_pc(struct thread_struct *t)
 {
 	extern void ret_from_fork(void);
 
diff --git a/include/asm-mips/system.h b/include/asm-mips/system.h
index b6d50e2f04..4992cbc5af 100644
--- a/include/asm-mips/system.h
+++ b/include/asm-mips/system.h
@@ -13,6 +13,9 @@
  * Kevin D. Kissell, kevink@mips.org and Carsten Langgaard, carstenl@mips.com
  * Copyright (C) 2000 MIPS Technologies, Inc.
  */
+
+/* extern -> static - cu570m */
+
 #ifndef _ASM_SYSTEM_H
 #define _ASM_SYSTEM_H
 
@@ -23,7 +26,7 @@
 #include <linux/kernel.h>
 #endif
 
-extern __inline__ void
+static __inline__ void
 __sti(void)
 {
 	__asm__ __volatile__(
@@ -47,7 +50,7 @@ __sti(void)
  * R4000/R4400 need three nops, the R4600 two nops and the R10000 needs
  * no nops at all.
  */
-extern __inline__ void
+static __inline__ void
 __cli(void)
 {
 	__asm__ __volatile__(
@@ -208,7 +211,7 @@ do { \
  * For 32 and 64 bit operands we can take advantage of ll and sc.
  * FIXME: This doesn't work for R3000 machines.
  */
-extern __inline__ unsigned long xchg_u32(volatile int * m, unsigned long val)
+static __inline__ unsigned long xchg_u32(volatile int * m, unsigned long val)
 {
 #ifdef CONFIG_CPU_HAS_LLSC
 	unsigned long dummy;
-- 
2.24.0

